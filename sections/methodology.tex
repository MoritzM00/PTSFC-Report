\newpage
\section{Methodology}
\label{ch:Methodology}

This chapter gives a detailed explanation of the methodology of this project. First, the pipeline is introduced in \cref{sec:Methodology:Pipeline}, followed by a description of the forecasting challenge in \cref{sec:Methodology:ForecastingChallenge}. Third, the evaluation in the context of probabilistic time-series forecasting is discussed in \cref{sec:Methodology:Evaluation}, and fourth, the feature engineering is presented in \cref{sec:Methodology:FeatureEngineering}.
Finally, the weekly forecasting challenge and how predictions are generated will be discussed in \cref{sec:Methodology:ForecastingChallenge}.


\subsection{Pipeline}
\label{sec:Methodology:Pipeline}

Typical data science projects are developed iteratively, making a reproducible and streamlined workflow essential for analysis and continuing improvement. For this reason, a data pipeline is implemented using \textit{Data Version Control} \parencite{ruslan_kuprieiev_dvc_2024}, providing automatic caching and execution mechanisms. This setup allows for comparing model metrics among experiments and ensures reproducibility because it tracks the state of the git repository along with user-defined configuration and stage outputs. An overview of the pipeline can be found in \cref{fig:pipeline}.
\begin{figure}[htbp]
  \centering
  \includesvg{images/graphviz.svg}
  \caption{Overview of pipeline stages with dependencies shown as a directed acyclic graph (DAG). There are four stages, but some are run several times. The prepare, train, and eval stages are run for each target indicated by the naming convention \textit{stage-name@target}.} 
  \label{fig:pipeline}
\end{figure}
The pipeline consists of four steps: First, the data is prepared by fetching relevant information from APIs. In this step, weather features using the OpenMeteo API are also generated. Second, models are trained and saved to disk for later use and inspection. \textit{Sktime} \parencite{franz_kiraly_sktimesktime_2024}, a popular python library for time series applications is used to provide an interface and avoid boilerplate code. Third, time-series cross-validation is performed, and all relevant metrics and visual inspections are calculated and plotted. More details on the evaluation are given in \cref{sec:Methodology:Evaluation}. Finally, out-of-sample forecasts for the specific forecasting horizons are generated and saved in the required format.

\subsection{Weekly Forecasting Challenge}
\label{sec:Methodology:ForecastingChallenge}

This project is centered around a weekly forecasting challenge, where participants submit
forecasts for their selected targets every Wednesday from October 23, 2024, until February 12, 2025. The weekly forecasts cover six specific forecast horizons. This section briefly describes how the weekly forecasts are calculated.

The currently selected model is trained on all available data. This happens during the \textit{train} stage of the pipeline, and each model is saved to disk. In the \textit{submit} stage, this model is loaded into memory again and used to generate forecasts in an autoregressive manner. This means that the model predicts one time-step $t$ ahead and then uses the median prediction to generate the prediction for time-step $t+1$. Throughout the challenge, the currently best model on the internal validation was selected, as it will be discussed in \cref{sec:Methodology:Evaluation}. After the challenge, models can be evaluated on the thirteen weeks similarly, which is done in \cref{sec:Results:Backtest}.

\subsection{Evaluation}
\label{sec:Methodology:Evaluation}

This section briefly describes how the models are evaluated using \textbf{Timeseries Cross-Validation} (TSCV) \parencite[Chapter~5.10]{hyndman_forecasting_2021}. The difference between standard Cross-Validation \parencite[241--248]{hastie_elements_2009} is that shuffling data points must be avoided because of data leakage. Furthermore, an expanding or a sliding window approach is employed. This procedure can be explained easily with a picture showing different training windows and forecasting horizons, as seen in \cref{fig:tscv}.
\begin{figure}[htbp]
    \centering
    \includesvg[width=\textwidth]{images/cv_window.svg}
    \caption{This figure depicts ten different folds in a Timeseries Cross-Validation with a sliding window. The train window (blue) uses a fixed amount of data points, in this case, three years, and then the following points after the train window ends is the test set (yellow), as defined by the forecasting horizon.}
    \label{fig:tscv}
\end{figure}
This project uses a sliding window approach to keep the training window constant. This is because the final model used for the submission forecasts will also be trained on roughly the same amount of data. At first, a constant three-year training window was selected. However, more data was significantly helpful for some models, especially for the Bike target, where less data is available due to daily bike counts. Therefore, the training window uses all available data, starting in October 2012 for Bikes and in October 2015 for Energy.

The different models will be compared using TSCV using 52 folds (weeks) starting from October 23, 2023, and ending right before the challenge begins on October 23, 2024. To keep the evaluation similar to the setup of the weekly forecasting challenge, a weekly forecasting horizon and window shifting are used. This means that models generate one-week ahead forecasts, after which the sliding window shifts by one week and the models are retrained to generate the forecasts for next week.

\textbf{Evaluation metrics} are computed for each TSCV fold and averaged over all folds. For this reason, all metrics computed during TSCV are reported with their corresponding standard deviation between folds. The main metric is the \textit{Pinball Loss} (also: Quantile Score) defined by:

\begin{equation}
\label{eq:PinballLoss}
\text{PB}_{\tau}(\hat{q}_{\tau}, y) = 
\begin{cases} 
(1 - \tau) (\hat{q}_{\tau} - y) & \hat{q}_{\tau} > y \\
\tau (y - \hat{q}_{\tau}) & y \geq \hat{q}_{\tau}
\end{cases}
,
\end{equation}
where $\hat{q}_\tau$ is the predictive $\tau$-quantile and y the observed value \parencite[Chapter~5.9]{hyndman_forecasting_2021}.

The Pinball Loss is computed for each quantile level $\tau \in \{0.025, 0.25, 0.5, 0.75, 0.975\}$, and then a sum over each Pinball Loss is computed. In addition to the Pinball Loss, which evaluates each quantile independently, interval metrics can be calculated involving two predictive quantiles. In this project, interval metrics are computed for the 50\% and 95\% central prediction interval (PI) as defined by the 25\%, 75\% quantiles and 2.5\%, 97.5\% quantiles, respectively. The \textit{interval score} (IS) can be computed directly from the two involved pinball losses with
\begin{equation}
\label{eq:IntervalScore}
\text{IS}_{\alpha}(l, u, y) = \frac{\text{PB}_{\alpha/2}(l, y) + \text{PB}_{1-\alpha/2}(u, y)}{\alpha}.
\end{equation}
where $l$ and $u$ are the quantiles corresponding to the lower and upper bounds of the prediction interval, respectively \parencite[370]{gneiting_strictly_2007}. 

In addition, the \textit{empirical coverage} of a prediction interval can be computed from 
observations $\{y_i\}_{i=1}^{n}$ using
\begin{equation}
    \text{Cvg}_{\alpha} = \frac{1}{n} \sum_{i = 1}^{n} \mathbf{1}\{ \hat{q}_{\alpha/2} \leq y_i \leq \hat{q}_{1 - \alpha /2 } \} \,,
\end{equation}
where $\mathbf{1}(\cdot)$ is the indicator function, which takes the value one if the condition is true and zero otherwise.
The coverage calculates the fraction of observations inside the interval and should equal $1 - \alpha$ to properly resemble a $(1 - \alpha)$-\% prediction interval.


\subsection{Feature Engineering}
\label{sec:Methodology:FeatureEngineering}

As most forecasting methods used in the project are traditional tabular regressors, 
it is essential to generate good feature representations. 
This is called feature engineering and will be discussed in more detail here.
The use of tabular regressors for forecasting is motivated by the availability of many potentially helpful features, such as weather variables, for energy consumption and bike traffic forecasting.
By incorporating these engineered features, we enhance the model's ability to 
capture complex temporal patterns and generate more accurate probabilistic forecasts.
In general, the features can be categorized into the following sections:

\textbf{Weather conditions} can significantly influence time series patterns, particularly in domains such as bike traffic. Therefore, weather-related variables such as temperature and precipitation are included. These features are derived from historical weather data using the OpenMeteo API and, where available, supplemented with forecasted values  \parencite{zippenfenig_open-meteocom_2024}.

\textbf{Seasonality} is especially important for the two datasets at hand because they show significant daily, weekly, and yearly patterns. To represent recurring patterns, we include cyclical encodings in the form of sine and cosine transformations or simply dummy variables, e.g., encoding the current month with eleven binary dummies \parencite[Chapter~7.4]{hyndman_forecasting_2021}.

\textbf{Lagged target values} provide historical context by introducing past observations as inputs to the model. We include short-term lags (e.g., 1-day or 1-hour) to capture immediate trends and long-term lags (e.g., same time on the previous week or month).

\textbf{Lagged exogenous variables} like weather variables can also be integrated but generally lead to many additional features. Therefore, the model has to somehow deal with the larger amount of features and select important lags.

\textbf{Holidays} often introduce irregular patterns in time series data. To account for these effects, we include a binary holiday indicator that marks whether a given time step corresponds to a public holiday or a significant event. This helps the model distinguish between typical and exceptional time periods, allowing it to adjust for fluctuations caused by holidays.

\textbf{Rolling-window computations}, e.g., the rolling median of the last 100 days is also often included for tabular regressors. In this project, the rolling median is motivated by the benchmark model. It is thus computed by taking the rolling median of all observations sharing the same weekday (and hour, if applicable).


