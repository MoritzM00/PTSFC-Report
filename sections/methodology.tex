\newpage
\section{Methodology}
\label{ch:Methodology}

This chapter gives a detailed explanation of the methodology of this project. The pipeline is introduced in \cref{sec:Methodology:Pipeline}, followed by a description of how models are evaluated internally in \cref{sec:Methodology:Evaluation}. Finally, the feature engineering process is discussed in \cref{sec:Methodology:FeatureEngineering}


\subsection{Pipeline}
\label{sec:Methodology:Pipeline}

Typical data science projects are developed iteratively, making a reproducible and streamlined workflow essential for analysis and continuing improvement. For this reason, a data pipeline is implemented using \textit{Data Version Control} \parencite{ruslan_kuprieiev_dvc_2024}, providing automatic caching and execution mechanisms. This setup allows for comparing model metrics among experiments and ensures reproducibility because it tracks the state of the git repository along with user-defined configuration and stage outputs. An overview of the stage can be found in \cref{fig:pipeline}.
\begin{figure}[htbp]
  \centering
  \includesvg{images/graphviz.svg}
  \caption{Overview of pipeline stages with dependencies shown as a directed acyclic graph (DAG). There are four stages, but some are run several times. The prepare, train, and eval stages are run for each target indicated by the naming convention \textit{stage-name@target}.} 
  \label{fig:pipeline}
\end{figure}
The pipeline consists of four steps: First, the data is prepared by fetching relevant information from APIs. In this step, weather features using the OpenMeteo API are also generated. Second, models are trained and saved to disk for later use and inspection. Third, time-series cross-validation is performed, and all relevant metrics and visual inspections are calculated and plotted. More details on the evaluation are given in \cref{sec:Methodology:Evaluation}. Finally, out-of-sample forecasts for the specific forecasting horizons are generated and saved in the required format.

\subsection{Evaluation}
\label{sec:Methodology:Evaluation}

This section briefly describes how the models are evaluated using Timeseries Cross-Validation (TSCV). The difference between standard Cross-Validation is that shuffling data points must be avoided because of data leakage. Furthermore, an expanding or a sliding window approach is employed. This procedure can be explained easily with a picture showing different training windows and forecasting horizons, as seen in \cref{fig:tscv}.
\begin{figure}[htbp]
    \centering
    \includesvg[width=\textwidth]{images/cv_window.svg}
    \caption{This figure depicts ten different folds in a Timeseries Cross-Validation with a sliding window. The train window (blue) uses a fixed amount of data points, in this case, three years, and then the following points after the train window ends is the test set (yellow), as defined by the forecasting horizon.}
    \label{fig:tscv}
\end{figure}
This project uses a sliding window approach to keep the training window constant. This is because the final model used for the submission forecasts will also be trained on roughly the same amount of data. The motivation for this is that more than three to four years of data might not be helpful anymore to predict one week into the future. In addition, COVID-19 effects are included in the training window for the final model if the window is extended further. 

For each fold in the TSCV, several metrics are computed and then averaged over all folds. All metrics are reported with their corresponding standard deviation between folds. The main metric is the Pinball Loss defined by:

\begin{equation}
\label{eq:PinballLoss}
\text{PB}_{\tau}(\hat{q}_{\tau}, y) = 
\begin{cases} 
(1 - \tau) (\hat{q}_{\tau} - y) & \hat{q}_{\tau} > y \\
\tau (y - \hat{q}_{\tau}) & y \geq \hat{q}_{\tau}
\end{cases}
,
\end{equation}
where $\hat{q}_\tau$ is the predictive $\tau$-quantile and y the observed value (NEEDS CITATION).

The Pinball Loss is computed for each quantile level $\tau \in \{0.025, 0.25, 0.5, 0.75, 0.975\}$, and then a sum over each Pinball Loss is computed. In addition to the Pinball Loss, which evaluates each quantile independently, interval metrics can be calculated involving two predictive quantiles. In this project, interval metrics are computed for the 50\% and 95\% central prediction interval (PI) as defined by the 25\%, 75\% quantiles and 2.5\%, 97.5\% quantiles, respectively. The interval score (IS) can be computed directly from the two involved pinball losses with
\begin{equation}
\label{eq:IntervalScore}
\text{IS}_{\alpha}(l, u, y) = \frac{\text{PB}_{\alpha/2}(l, y) + \text{PB}_{1-\alpha/2}(u, y)}{\alpha}.
\end{equation}
where $l$ and $u$ are the quantiles corresponding to the lower and upper bounds of the prediction interval, respectively. (NEEDS CITATION)

In addition, the empirical coverage of a prediction interval can be computed from 
observations $\{y_i\}_{i=1}^{n}$ using
\begin{equation}
    \text{Cvg}_{\alpha} = \frac{1}{n} \sum_{i = 1}^{n} \mathbbm{1}\{ \hat{q}_{\alpha/2} \leq y_i \leq \hat{q}_{1 - \alpha /2 } \} \,,
\end{equation}
where $\mathbbm{1}(\cdot)$ is the indicator function, which takes the value one if the condition is true and zero otherwise.
The coverage calculates the fraction of observations inside the interval and should equal $1 - \alpha$ to properly resemble a $(1 - \alpha)$-\% prediction interval.


\subsection{Feature Engineering}
\label{sec:Methodology:FeatureEngineering}

As most forecasting methods used in the project are traditional tabular regressors, 
it is essential to generate good feature representations. 
This is called feature engineering and will be discussed in more detail here.
The use of tabular regressors for forecasting is motivated by the availability of many potentially helpful features, such as weather variables, for energy consumption and bike traffic forecasting.
By incorporating these engineered features, we enhance the model's ability to 
capture complex temporal patterns and generate more accurate probabilistic forecasts.
In general, the features can be categorized into the following sections:

\textbf{Weather conditions} can significantly influence time series patterns, particularly in domains such as bike traffic. Therefore, weather-related variables such as temperature and precipitation are included. These features are derived from historical weather data using the OpenMeteo API and, where available, supplemented with forecasted values  \parencite{zippenfenig_open-meteocom_2024}.

\textbf{Seasonality} is especially important for the two datasets at hand because they show significant daily, weekly, and yearly patterns. To represent recurring patterns, we include cyclical encodings in the form of sinus and cosinus transformations or simply dummy variables, e.g. encoding the current month with eleven binary dummies.
% For a given time-based variable \( t \) with a periodicity of \( P \), we compute two features:
% \begin{align}
%     \text{sin\_encoding} &= \sin\left( \frac{2\pi t}{P} \right) \\
%     \text{cos\_encoding} &= \cos\left( \frac{2\pi t}{P} \right)
% \end{align}

\textbf{Lagged target values} provide historical context by introducing past observations as inputs to the model. We include both short-term lags (e.g., 1-day or 1-hour lag) to capture immediate trends and long-term lags (e.g., same time on the previous week or month).

\textbf{Lagged exogenous variables} like weather variables can also be integrated but generally lead to many additional features. Therefore, the model has to somehow deal with the larger amount of features and select important lags.

\textbf{Holidays} often introduce irregular patterns in time series data. To account for these effects, we include a binary holiday indicator that marks whether a given time step corresponds to a public holiday or a significant event. This helps the model distinguish between typical and exceptional time periods, allowing it to adjust for fluctuations caused by holidays.


\subsection{Weekly Forecasting Challenge}
\label{sec:Methodology:ForecastingChallenge}

This project is centered around a weekly forecasting challenge, where participants submit
forecasts for their selected targets every Wednesday from October 23, 2024, until February 12, 2025. The weekly forecasts cover six specific forecast horizons. This section briefly describes how the weekly forecasts are calculated.

First, the currently selected model is trained on all available data for both targets to partially exclude effects from the pandemic period and keep the training window similar to the one used during the evaluation.\footnote{At the beginning, only a three-year training window was selected but a longer training window seemed to help, especially for the bike target where less data is available due to daily counts.} This happens during the \textit{train} stage of the pipeline, and each model is saved to disk. In the \textit{submit} stage, this model is loaded into memory again and used to generate forecasts.
A wider evaluation window 
was used to evaluate models during the challenge to keep the evaluation consistent as the weekly forecasts progressed. However, after the challenge was finished, a backtest using the final version of each model was done to evaluate models on the specific dates of the forecasting challenge.


