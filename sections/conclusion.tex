%% LaTeX2e class for seminar theses
%% sections/conclusion.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
\newpage
\section{Conclusion}
\label{ch:Conclusion}

Gradient-Boosting Decision Trees have demonstrated state-of-the-art performance across various benchmarks and frequently surpass neural networks, particularly in data-limited settings \parencites{shwartz-ziv_tabular_2021, grinsztajn_why_2022}. This study conducted a comparative analysis of three widely used Gradient-Boosting Decision Tree libraries—\textsc{LightGBM}, \textsc{XGBoost}, and \textsc{CatBoost}—within the context of probabilistic forecasting for two distinct targets. The first target, \textit{Energy}, involved forecasting hourly energy demand in Germany, while the second, \textit{Bikes}, focused on predicting the daily bicycle count in Karlsruhe. Model performance was assessed using time series cross-validation to obtain reliable error estimates, benchmarked against two baseline models: a historical quantile-based approach and linear quantile regression (cf. \cref{sec:Models:Baselines}).

The implementation was carried out using \textsc{sktime}, a leading Python library for time series forecasting. Alternative libraries, such as \textsc{Darts} \parencite{herzen_darts_2022} and \textsc{Nixtla}’s forecasting suite (e.g., \textsc{StatsForecast} \parencite{azul_garza_statsforecast_2022}), were considered. However, \textsc{sktime} was selected due to its well-structured interface and superior support for probabilistic forecasting. Despite these advantages, \textsc{sktime} has notable limitations, including incomplete documentation and a less intuitive integration of tabular regression models. To address these challenges, custom wrappers were developed to align the models with the \textsc{sktime} framework, ensuring greater flexibility and consistency in implementation.

All GBDT models outperformed the two baseline models by a significant margin. However, differences among the various GBDT implementations were marginal, particularly when considering the relatively high standard deviation. It is important to note that GBDT models exhibit high sensitivity to hyperparameters, making tuning a non-trivial task that involves balancing generalization ability and bias. Notably, Ordered CatBoost yielded the worst results among the GBDT models and required the longest training time. However, employing the plain algorithm in CatBoost led to a slight improvement over LightGBM on the Energy target. 
XGBoost performed slightly worse on both targets and exhibited longer training times compared to LightGBM. The latter consistently demonstrated strong performance and was the fastest among the three GBDT implementations evaluated. Future research could explore hyperparameter tuning in greater depth by leveraging advanced optimization techniques, such as those available in the Optuna library \parencite{akiba_optuna_2019} in accordance with several studies that have explored optimal hyperparameter tuning strategies for GBDT models \parencites{florek_benchmarking_2023, bentejac_comparative_2021}.

While hyperparameter optimization typically yields only marginal improvements, refining feature representations can significantly enhance model performance. Consequently, several exogenous variables, including weather data and seasonal encodings, were incorporated. A particularly influential feature for both targets was the binary indicator for public holidays. Weather variables proved beneficial for the Bikes target but slightly deteriorated performance for the Energy target. This may be attributed to the inclusion of weather data solely from Karlsruhe. Future research could explore the impact of incorporating weather data averaged across multiple stations nationwide. Additionally, this study did not investigate model averaging, given that GBDT models are inherently strong ensemble learners. However, further improvements could potentially be achieved by averaging the quantile predictions of the three GBDT implementations.

