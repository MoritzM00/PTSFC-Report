%% LaTeX2e class for seminar theses
%% sections/conclusion.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
\newpage
\section{Conclusion}
\label{ch:Conclusion}
Gradient-Boosting Decision Trees achieve state-of-the-art results on many benchmarks and often outperform neural networks -- especially if training data is limited \parencites{shwartz-ziv_tabular_2021, grinsztajn_why_2022}. Therefore, a comparative study of the three popular Gradient-Boosting Decision Tree libraries \textsc{LightGBM}, \textsc{XGBoost}, and \textsc{CatBoost} in the context of probabilistic forecasting for two distinct targets was conducted. The first target, \textit{Energy}, was to forecast the hourly energy demand in Germany. The second target, \textit{Bikes}, focused on predicting the daily bicycle count in Karlsruhe. Models were evaluated using time series cross-validation for a more accurate estimate of the model errors against two baseline models. The baselines consisted of a historical quantile-based approach and a linear quantile regression (cf. \cref{sec:Models:Baselines}).

The methods were implemented using \textsc{sktime}, one of the biggest libraries for time series forecasting in the Python ecosystem. Alternative libraries such as \textsc{Darts} \parencite{herzen_darts_2022} and the forecasting suite by \textsc{Nixtla} (e.g. \textsc{StatsForecast} \parencite{azul_garza_statsforecast_2022}) were considered as well, but ultimately, Sktime was chosen due to its well-structured interface and superior support for probabilistic forecasting. Despite these advantages, \textsc{sktime} has notable limitations, including incomplete documentation and a less intuitive integration of tabular regression models. Therefore, the methods were implemented with custom wrappers using the Sktime interface to allow for greater flexibility.

All GBDT models outperformed the two baseline models by a significant margin. However, differences among the various GBDT implementations were marginal, particularly when considering the relatively high standard deviation. It is important to note that GBDT models exhibit high sensitivity to hyperparameters, making tuning a non-trivial task that involves balancing generalization ability and bias. Some experimental studies investigate how hyperparameters should be tuned for GDBT models \parencites{florek_benchmarking_2023, bentejac_comparative_2021}. For example, the parameter for the boosting type in CatBoost has a significant impact: Ordered CatBoost performed the worst among GBDT models by a wide margin and took the longest to train. However, using regular Gradient-Boosting in CatBoost even outperformed LightGBM slightly on the Energy Target. XGBoost was somewhat worse on both targets and needed more time to train than LightGBM. The latter performed well consistently and was the fastest implementation of the three GBDT libraries discussed here. Future work could explore hyperparameter tuning in greater depth by leveraging advanced optimization techniques, such as those available in the Optuna library \parencite{akiba_optuna_2019}.

Hyperparameter optimization mostly leads to marginal gains, but focusing on feature representations can improve performance dramatically. Therefore, several exogenous variables, such as weather variables and encodings for seasonality, were incorporated. The binary indicator for public holidays was a particularly important variable for both targets. Weather variables proved beneficial for the Bike Target but slightly decreased the performance for the Energy Target. This could be due to the fact that only weather variables for Karlsruhe were included, while Germany's consumption had to be predicted. Here, improvement could be made by using weather data averaged at multiple stations across the country. In addition, this study did not investigate model averaging, as GBDT models are inherently strong ensemble learners. Further improvements could be achieved by averaging the quantile predictions of the three GBDT implementations.

