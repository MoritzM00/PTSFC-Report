%% LaTeX2e class for seminar theses
%% sections/conclusion.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
\newpage
\section{Conclusion}
\label{ch:Conclusion}

Gradient-Boosting Decision Trees achieve state-of-the-art results on many benchmarks and often outperform neural networks -- especially if training data is limited \parencites{shwartz-ziv_tabular_2021, grinsztajn_why_2022}. Therefore, a comparative study of the three popular Gradient-Boosting Decision Tree libraries \textsc{LightGBM}, \textsc{XGBoost}, and \textsc{CatBoost} in the context of probabilistic forecasting for two targets was employed. The first target, Energy, was to forecast the hourly energy demand in Germany. The second target, Bikes, focused on predicting the daily bicycle count in Karlsruhe. Models were evaluated using time series cross-validation for a more accurate estimate of the model errors against two baseline models. The baselines consisted of a historical quantile-based approach and a linear quantile regression (cf. \cref{sec:Models:Baselines}).

The methods were implemented using \textsc{sktime}, which is one of the biggest libraries for time series forecasting in the Python ecosystem. Other Libraries like \textsc{Darts} \parencite{herzen_darts_2022} and the forecasting suite by \textsc{Nixtla} (e.g. \textsc{StatsForecast} \parencite{azul_garza_statsforecast_2022}) were considered as well, but ultimately, Sktime was chosen due to it providing the greatest flexibility through a well-designed interface and the best support for probabilistic forecasting. However, it is far from perfect due to limited documentation and the particular way of how tabular regression models are integrated. Therefore, the methods were implemented with custom wrappers using the Sktime interface.

All GBDT models outperformed the two baseline models by a good margin, although differences between various GBDT implementations were marginal when taking the rather high standard deviation into account. It must be noted that GBDT models are very sensitive to hyperparameters and tuning them is often not trivial, requiring a trade-off between generalization ability and bias. Some experimental studies investigate how hyperparameters should be tuned for GDBT models \parencites{florek_benchmarking_2023, bentejac_comparative_2021}. For example, the parameter for the boosting type in CatBoost has a significant impact: Ordered CatBoost performed the worst among GBDT models by a wide margin and took the longest to train. However, using regular Gradient-Boosting in CatBoost even outperformed LightGBM slightly on the Energy Target. XGBoost was somewhat worse on both targets and needed more time to train than LightGBM. The latter performed well consistently and was the fastest implementation of the three GBDT libraries discussed here. In future work, hyperparameter-tuning could be investigated more thoroughly using advanced optimization techniques implemented e.g., by the popular \textit{Optuna} library \parencite{akiba_optuna_2019}.

However, hyperparameter optimization mostly leads to marginal gains, but focusing on feature representations can improve the performance dramatically. Therefore, several exogenous variables, such as weather variables and encodings for seasonality, were used. A very important variable for both targets was the binary indicator for public holidays. Weather variables were helpful for the Bike Target but slightly decreased the performance for the Energy Target. This could be due to the fact that only weather variables for Karlsruhe were included, while the consumption of Germany has to be predicted. Here, improvement can potentially be seen by using weather averaged over several stations throughout Germany. In addition, model averaging has not been looked into in this report because the individual models are already strong ensemble models. Further improvements could be made by averaging the quantile predictions of the three GBDT models.

